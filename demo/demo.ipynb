{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda3/envs/rescore/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-19 00:25:13,123\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    Indexer     ] Deserializing Index from ./data/database/contriever_msmarco/musique...  Deserializing Complete!\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from source.pipeline.config import PipelineConfig\n",
    "from source.pipeline.controller import PipelineController\n",
    "from source.pipeline.step.retrieval import RetrievalStep\n",
    "from source.pipeline.step.generation import (\n",
    "    GenerationStep, \n",
    "    AnswerGenerateOutputParser, \n",
    "    AnswerGeneratePromptGenerator,\n",
    "    ThoughtGenerateOutputParser,\n",
    "    ThoughtGeneratePromptGenerator,\n",
    ")\n",
    "from source.pipeline.step.end import EndStep\n",
    "from source.pipeline.state import QuestionState\n",
    "from source.module.generate.llama import LlamaGenerator, LlamaGeneratorConfig\n",
    "from source.module.retrieve.dense import DenseRetriever, DenseRetrieverConfig\n",
    "from source.module.index.index import Indexer, IndexerConfig\n",
    "from source.utility.system_utils import seed_everything\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=f\"{your_hf_token}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "seed_everything(100)\n",
    "\n",
    "cfg = PipelineConfig(\n",
    "    method=\"base\",\n",
    "    batch_size=1,\n",
    "    generation_model_name='meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
    "    generation_max_batch_size=1,\n",
    "    generation_max_total_tokens=4096,\n",
    "    generation_max_new_tokens=64,\n",
    "    generation_min_new_tokens=1,\n",
    "    retrieval_count=8,\n",
    "    retrieval_query_type='full',\n",
    "    dataset='musique',\n",
    "    max_num_thought=6,\n",
    "    answer_regex=\".* answer is:? (.*)\\\\.?\",\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Module Init\n",
    "# ----------------------------\n",
    "generator = LlamaGenerator(\n",
    "    LlamaGeneratorConfig(\n",
    "        model_name=cfg.generation_model_name,\n",
    "        batch_size=cfg.generation_max_batch_size,\n",
    "        max_total_tokens=cfg.generation_max_total_tokens,\n",
    "        max_new_tokens=cfg.generation_max_new_tokens,\n",
    "        min_new_tokens=cfg.generation_min_new_tokens,\n",
    "        use_vllm=False, #True,\n",
    "        gpu=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "retriever = DenseRetriever(\n",
    "    DenseRetrieverConfig(\n",
    "        query_model_name_or_path='facebook/contriever-msmarco',\n",
    "        passage_model_name_or_path=None,\n",
    "        batch_size=32,\n",
    "        training_strategy=None,\n",
    "        use_fp16=False\n",
    "    )\n",
    ")\n",
    "\n",
    "indexer = Indexer.load_local(\n",
    "    IndexerConfig(\n",
    "        embedding_sz=768,\n",
    "        database_path=cfg.database_path\n",
    "    )\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Pipeline & Controller Setup\n",
    "# ----------------------------\n",
    "pipeline = [\n",
    "    RetrievalStep(cfg=cfg, retriever=retriever, indexer=indexer),\n",
    "    GenerationStep(cfg=cfg, generator=generator,\n",
    "                   prompt_generator=AnswerGeneratePromptGenerator(cfg),\n",
    "                   output_parser=AnswerGenerateOutputParser(cfg)),\n",
    "    EndStep(cfg=cfg),\n",
    "    GenerationStep(cfg=cfg, generator=generator,\n",
    "                   prompt_generator=ThoughtGeneratePromptGenerator(cfg),\n",
    "                   output_parser=ThoughtGenerateOutputParser(cfg)),\n",
    "]\n",
    "\n",
    "controller = PipelineController(\n",
    "    pipeline=pipeline,\n",
    "    logging_file_path=None,\n",
    "    prediction_file_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=\"Which missionary helped spread the religion widely practiced in region having the second largest rain-forest in the world?\"\n",
    "\n",
    "start_state = QuestionState(question_id=\"1\", question=user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=\"Which missionary helped spread the religion widely practiced in region having the second largest rain-forest in the world?\"\n",
    "\n",
    "start_state = QuestionState(question_id=\"1\", question=user_input)\n",
    "\n",
    "controller.update([start_state])\n",
    "paths = controller.next()  \n",
    "# 1st-hop Retrieve\n",
    "next_states = controller.pipeline[0](paths) \n",
    "for d, document in enumerate(next_states[0].documents[:8]):\n",
    "    print(document.metadata['title'])\n",
    "controller.update(next_states)\n",
    "paths = controller.next()\n",
    "# 1st-hop Answer\n",
    "next_states = controller.pipeline[1](paths) \n",
    "print(f\"Model Prediction: {next_states[0].answer}\")\n",
    "# if \"Unknown\" continue else exit\n",
    "controller.update(next_states)\n",
    "paths = controller.next()\n",
    "# 1st-hop Check Answer\n",
    "next_states = controller.pipeline[2](paths) \n",
    "controller.update(next_states)\n",
    "paths = controller.next()\n",
    "# 1st-hop Think\n",
    "next_states = controller.pipeline[3](paths) \n",
    "print(f\"Intermediate Thought: {next_states[0].thought}\")\n",
    "controller.update(next_states)\n",
    "paths = controller.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Loop for hop >= 2 ----\n",
    "MAX_HOPS = 5\n",
    "hop = 2\n",
    "\n",
    "while hop <= MAX_HOPS:\n",
    "    # Retrieve\n",
    "    next_states = controller.pipeline[0](paths)\n",
    "    # TODO: we use a buffer of 32 to remove redundant documents, this is not implemented in this demo\n",
    "    titles = [doc.metadata['title'] for doc in next_states[0].documents[:8]]\n",
    "    print(f\"Retrieved: {titles}\") \n",
    "    controller.update(next_states)\n",
    "    paths = controller.next()\n",
    "\n",
    "    # Answer\n",
    "    next_states = controller.pipeline[1](paths)\n",
    "    print(f\"{hop}-hop Answer: {next_states[0].answer}\")\n",
    "\n",
    "    controller.update(next_states)\n",
    "    paths = controller.next()\n",
    "\n",
    "    if next_states[0].answer != \"Unknown\":\n",
    "        break\n",
    "\n",
    "    # Check\n",
    "    next_states = controller.pipeline[2](paths)\n",
    "    controller.update(next_states)\n",
    "    paths = controller.next()\n",
    "\n",
    "    # Think\n",
    "    next_states = controller.pipeline[3](paths)\n",
    "    print(f\"{hop}-hop Thought: {next_states[0].thought}\")\n",
    "\n",
    "    controller.update(next_states)\n",
    "    paths = controller.next()\n",
    "\n",
    "    hop += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rescore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
